## 1

![](https://github.com/zee-nguyen/Stanford_Machine_Learning/blob/master/assets/W5/W5_Quiz_Q1.png?raw=true)

## 2

![](https://github.com/zee-nguyen/Stanford_Machine_Learning/blob/master/assets/W5/W5_Quiz_Q2.png?raw=true)

Theta 1 has 15 elements, so Theta2 starts from 16. Total sum of elements is 39 so we go from 16:39 and get 4x6.

## 3

![](https://github.com/zee-nguyen/Stanford_Machine_Learning/blob/master/assets/W5/W5_Quiz_Q3.png?raw=true)

Plug in theta and epsilon then apply J(theta) to get 12.0012.

## 4

![](https://github.com/zee-nguyen/Stanford_Machine_Learning/blob/master/assets/W5/W5_Quiz_Q4.png?raw=true)

**Missing correct answers for this question**

A. Gradient checking is used to verify that BP works. It doesn't depend on which algorithm we're using.

B. This is precisely why we're using gradient checking.

C. Increasing the regularization parameter would result in underfitting??

D.

## 5

![](https://github.com/zee-nguyen/Stanford_Machine_Learning/blob/master/assets/W5/W5_Quiz_Q5.png?raw=true)

A. Initialize all the params to ones instead of zeros doesn't change anything.

B.

C.

D
